{
    "tools": [
        {
            "name": "evaluate_response",
            "description": "\n    Evaluate an LLM response using Atla's evaluation model.\n\n    This function takes evaluation parameters and sends them to the Atla API for evaluation.\n    It returns a dictionary containing the evaluation score and critique.\n\n    Args:\n        model_input (str): The prompt or question given to the model to generate the response.\n        model_output (str): The response generated by the model that needs to be evaluated.\n        evaluation_criteria (str): The specific criteria or instructions for evaluating the model's output.\n        expected_model_output (Optional[str]): A reference or ideal answer to compare against the model's output. Defaults to None.\n        model_context (Optional[str]): Additional context or information provided to the model during generation. Defaults to None.\n        model_id (str): The identifier of the Atla evaluation model to use. Defaults to \"atla-selene\".\n\n    Returns:\n        Dict[str, str]: A dictionary containing two keys:\n            - \"score\": The numerical evaluation score assigned by the Atla model.\n            - \"critique\": A textual explanation or critique of the evaluation.\n    ",
            "inputSchema": {
                "type": "object",
                "properties": {
                    "model_input": {
                        "type": "string",
                        "description": "The prompt or question given to the model to generate the response."
                    },
                    "model_output": {
                        "type": "string",
                        "description": "The response generated by the model that needs to be evaluated."
                    },
                    "evaluation_criteria": {
                        "type": "string",
                        "description": "The specific criteria or instructions for evaluating the model's output."
                    },
                    "expected_model_output": {
                        "type": "string",
                        "description": "A reference or ideal answer to compare against the model's output. Defaults to None."
                    },
                    "model_context": {
                        "type": "string",
                        "description": "Additional context or information provided to the model during generation. Defaults to None."
                    },
                    "model_id": {
                        "type": "string",
                        "description": "The identifier of the Atla evaluation model to use. Defaults to \"atla-selene\"."
                    },
                    "Returns": {
                        "type": "string"
                    }
                },
                "required": [
                    "model_input",
                    "model_output",
                    "evaluation_criteria",
                    "model_id",
                    "Returns"
                ]
            },
            "file": "atla-mcp-server.py",
            "decorator": [
                "mcp.tool"
            ]
        },
        {
            "name": "batch_evaluate_responses",
            "description": "\n    Perform batch evaluation of multiple LLM responses using Atla's evaluation model.\n\n    This function takes a list of evaluation requests and processes them concurrently using asyncio.\n    It returns a list of evaluation responses.\n\n    Args:\n        evaluations (List[Dict[str, Any]]): A list of dictionaries, each containing the parameters for a single evaluation:\n            - model_input (str): The prompt or question given to the model.\n            - model_output (str): The response generated by the model.\n            - evaluation_criteria (str): The criteria for evaluation.\n            - expected_model_output (Optional[str]): A reference answer (if available).\n            - model_context (Optional[str]): Additional context for the model.\n            - model_id (str): The Atla model ID to use for evaluation.\n\n    Returns:\n        List[Dict[str, str]]: A list of dictionaries, each containing the evaluation results:\n            - \"score\": The numerical evaluation score.\n            - \"critique\": A textual explanation of the evaluation.\n    ",
            "inputSchema": {
                "type": "object",
                "properties": {
                    "evaluations": {
                        "type": "string",
                        "description": "A list of dictionaries, each containing the parameters for a single evaluation:"
                    },
                    "Returns": {
                        "type": "string"
                    }
                },
                "required": [
                    "evaluations",
                    "Returns"
                ]
            },
            "file": "atla-mcp-server.py",
            "decorator": [
                "mcp.tool"
            ]
        },
        {
            "name": "list_metrics",
            "description": "\n    List available metrics using Atla's SDK.\n\n    This function retrieves a list of available metrics from the Atla API.\n    Each metric is returned as a dictionary containing its properties.\n\n    Returns:\n        List[Dict[str, str]]: A list of dictionaries, each representing a metric with the following keys:\n            - \"id\": The unique identifier of the metric.\n            - \"name\": The name of the metric.\n            - \"description\": A brief description of what the metric measures or evaluates.\n    ",
            "inputSchema": {
                "type": "object",
                "properties": {}
            },
            "file": "atla-mcp-server.py",
            "decorator": [
                "mcp.tool"
            ]
        },
        {
            "name": "create_metric",
            "description": "\n    Create a new custom metric using Atla's SDK.\n\n    This function creates a new metric with the specified parameters, adds a prompt,\n    and sets it as the active version. It returns the ID of the newly created metric.\n\n    Args:\n        name (str): The name of the new metric.\n        metric_type (str): The type of the metric (e.g., \"likert_1_to_5\" or \"likert_0_to_1\").\n        prompt (str): The evaluation prompt or instructions for using this metric.\n        description (Optional[str]): A brief description of what the metric measures. Defaults to None.\n\n    Returns:\n        str: The unique identifier (ID) of the newly created metric.\n    ",
            "inputSchema": {
                "type": "object",
                "properties": {
                    "name": {
                        "type": "string",
                        "description": "The name of the new metric."
                    },
                    "metric_type": {
                        "type": "string",
                        "description": "The type of the metric (e.g., \"likert_1_to_5\" or \"likert_0_to_1\")."
                    },
                    "prompt": {
                        "type": "string",
                        "description": "The evaluation prompt or instructions for using this metric."
                    },
                    "description": {
                        "type": "string",
                        "description": "A brief description of what the metric measures. Defaults to None."
                    },
                    "Returns": {
                        "type": "string"
                    },
                    "str": {
                        "type": "string",
                        "description": "The unique identifier (ID) of the newly created metric."
                    }
                },
                "required": [
                    "name",
                    "metric_type",
                    "prompt",
                    "Returns",
                    "str"
                ]
            },
            "file": "atla-mcp-server.py",
            "decorator": [
                "mcp.tool"
            ]
        },
        {
            "name": "get_metric_by_name",
            "description": "\n    Retrieve a metric by its name using Atla's SDK.\n\n    This function searches for a metric with the given name in the list of available metrics.\n    If found, it returns the metric's details; otherwise, it raises a ValueError.\n\n    Args:\n        name (str): The name of the metric to retrieve.\n\n    Returns:\n        Dict[str, str]: A dictionary containing the metric's details:\n            - \"id\": The unique identifier of the metric.\n            - \"name\": The name of the metric.\n            - \"description\": A brief description of what the metric measures.\n\n    Raises:\n        ValueError: If a metric with the given name is not found.\n    ",
            "inputSchema": {
                "type": "object",
                "properties": {
                    "name": {
                        "type": "string",
                        "description": "The name of the metric to retrieve."
                    },
                    "Returns": {
                        "type": "string"
                    },
                    "Raises": {
                        "type": "string"
                    },
                    "ValueError": {
                        "type": "string",
                        "description": "If a metric with the given name is not found."
                    }
                },
                "required": [
                    "name",
                    "Returns",
                    "Raises",
                    "ValueError"
                ]
            },
            "file": "atla-mcp-server.py",
            "decorator": [
                "mcp.tool"
            ]
        }
    ]
}